{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPniSZsv9btAeC1MMo85UX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SSRavipati/LLM-course/blob/main/chapter_2/Handling_Multiple_sequence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Handling multiple sequences**"
      ],
      "metadata": {
        "id": "IPwDBiOu9FVx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DU5M0NNU9Dp0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"first gear low speed\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = torch.tensor(ids)\n",
        "# This line will fail.\n",
        "model(input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code we can see the code failed as the transoformer was expecting multiple sequences"
      ],
      "metadata": {
        "id": "NQdgGR_M-SG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(sequence, padding = True, truncation = True, return_tensors = \"pt\")\n",
        "print(inputs)\n",
        "model(**inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACvGeUsM975g",
        "outputId": "5034b732-bc15-41bd-9c72-bff28ecd2d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
            "          2607,  2026,  2878,  2166,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code had no errors in this case, we can gather that tokenizer adds a dimension along with tokinizing and encoding"
      ],
      "metadata": {
        "id": "TU3x7Iix-ZVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s try again and add a new dimension:\n"
      ],
      "metadata": {
        "id": "iBS3FaNJ_ZB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"first gear is low speed\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "input_ids = torch.tensor([ids])\n",
        "# here we are creating a 2D  tensor\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyHT88op9PqM",
        "outputId": "73ff852f-b1c9-49d3-9b25-6f37a481af33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tensor([[2034, 6718, 2003, 2659, 3177]])\n",
            "Logits: tensor([[-0.6330,  0.6272]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sentences = [\n",
        "    \"I’ve been waiting for this my whole life.\",\n",
        "    \"I hate this so much!\"\n",
        "]\n",
        "\n",
        "# Manually tokenize and convert to IDs for each sentence\n",
        "ids = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence)) for sentence in sentences]\n",
        "\n",
        "# Find the maximum sequence length in the batch\n",
        "max_len = max(len(sublist) for sublist in ids)\n",
        "\n",
        "# Get the padding token ID\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "if pad_token_id is None:\n",
        "\n",
        "    pad_token_id = 0\n",
        "    print(f\"Warning: Tokenizer does not have a defined pad_token_id. Using {pad_token_id} for padding.\")\n",
        "\n",
        "\n",
        "# Manually pad the sequences and create the attention mask\n",
        "padded_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for sublist in ids:\n",
        "    # Calculate how many padding tokens are needed\n",
        "    padding_length = max_len - len(sublist)\n",
        "\n",
        "    # Pad the sequence\n",
        "    padded_sequence = sublist + [pad_token_id] * padding_length\n",
        "    padded_ids.append(padded_sequence)\n",
        "\n",
        "    # Create the attention mask: 1 for original tokens, 0 for padding tokens\n",
        "    attention_mask = [1] * len(sublist) + [0] * padding_length\n",
        "    attention_masks.append(attention_mask)\n",
        "\n",
        "# Convert the padded sequences and attention masks to PyTorch tensors\n",
        "input_ids = torch.tensor(padded_ids)\n",
        "attention_mask = torch.tensor(attention_masks)\n",
        "\n",
        "print(\"Input IDs (padded):\", input_ids)\n",
        "print(\"Attention Mask:\", attention_mask)\n",
        "\n",
        "# Pass the tensors through the model\n",
        "# The model expects input_ids and attention_mask as keyword arguments\n",
        "output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "print(\"Logits:\", output.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CKU_uym9PRv",
        "outputId": "10fe6107-cbf1-4e62-a3dd-405ebaa7a756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs (padded): tensor([[1045, 1521, 2310, 2042, 3403, 2005, 2023, 2026, 2878, 2166, 1012],\n",
            "        [1045, 5223, 2023, 2061, 2172,  999,    0,    0,    0,    0,    0]])\n",
            "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])\n",
            "Logits: tensor([[-3.2906,  3.4655],\n",
            "        [ 3.1931, -2.6685]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    }
  ]
}