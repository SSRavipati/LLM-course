{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVqCjFIvSTjQIsElK1FXTN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SSRavipati/LLM-course/blob/main/chapter_2/Data_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Processing the data**"
      ],
      "metadata": {
        "id": "Kq5qKYhz5InS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets fsspec"
      ],
      "metadata": {
        "id": "TQOTGtN56Ljs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh4j3RWt5HWC"
      },
      "outputs": [],
      "source": [
        "# from datasets import load_dataset\n",
        "# raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_dataset = raw_datasets[\"train\"]\n",
        "print(raw_train_dataset[0])\n",
        "raw_train_dataset.features\n",
        "print(raw_train_dataset[ : 5])\n",
        "raw_test_dataset = raw_datasets[\"test\"]\n",
        "raw_test_dataset[0]\n",
        "raw_validation_dataset = raw_datasets[\"validation\"]\n",
        "raw_validation_dataset[0]"
      ],
      "metadata": {
        "id": "5LdhlP5K7DxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_train_dataset[15]['label'])\n",
        "print(raw_validation_dataset[87]['label'])"
      ],
      "metadata": {
        "id": "mMwSCDuN7mJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "a7-oQmxu_ae9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "checkpoint = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "raw_sentence1 = raw_train_dataset[15]['sentence1']\n",
        "raw_sentence2 = raw_train_dataset[15]['sentence2']\n",
        "print(raw_sentence1)\n",
        "print(raw_sentence2)\n",
        "inputs1 = tokenizer(raw_sentence1)\n",
        "inputs2= tokenizer(raw_sentence2)\n",
        "print(inputs1)\n",
        "print(inputs2)\n",
        "\n",
        "inputs = tokenizer(raw_sentence1, raw_sentence2)\n",
        "print(inputs)\n",
        "# we can see that there are token_typeids in the inputs it is because bert was trained on them"
      ],
      "metadata": {
        "id": "lAYGdGak8OZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenizer(\n",
        "    raw_datasets[\"train\"][\"sentence1\"],\n",
        "    raw_datasets[\"train\"][\"sentence2\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "SlvIs-kT-OOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
      ],
      "metadata": {
        "id": "q4iJtHYPBuCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "MhAksgC0Bwz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dynamic Padding**"
      ],
      "metadata": {
        "id": "ICOqKhD3C4G5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   In the above case we deliberately did not use padding, now all the sequences are of various lengths, but they are batched\n",
        "*   In batches we try for the sentences in the batch to match the longest sentence in the batch\n",
        "* *this is advantageous when using cpus and gpus but NOT WITH TPUS, there we need all batches of same shape*\n",
        "*   The function that is responsible for putting together samples inside a batch is called a collate function. Itâ€™s an argument you can pass when you build a DataLoader\n",
        "\n"
      ],
      "metadata": {
        "id": "sj7_-sSQEXI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do this in practice, we have to define a collate function that will apply the correct amount of padding to the items of the dataset we want to batch together. Transformers library provides us with such a function via DataCollatorWithPadding."
      ],
      "metadata": {
        "id": "xKWpEnYXFdG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "lv7MSffDCm8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = tokenized_datasets[\"train\"][:8]\n",
        "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
        "print(samples)\n",
        "[len(x) for x in samples[\"input_ids\"]]"
      ],
      "metadata": {
        "id": "9I6EufNJFkaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = data_collator(samples)\n",
        "{k: v.shape for k, v in batch.items()}"
      ],
      "metadata": {
        "id": "E60QsOmuFozE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets2 = load_dataset(\"GLUE\", \"sst2\")\n",
        "raw_datasets2"
      ],
      "metadata": {
        "id": "D4LYd2ySF4AY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets2[\"train\"][0]"
      ],
      "metadata": {
        "id": "brB-b-JShVTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function2(example):\n",
        "  return tokenizer(example['sentence'], truncation = True)"
      ],
      "metadata": {
        "id": "QK3l5CZBg1Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets2 = raw_datasets2.map(tokenize_function2, batched = True)\n",
        "tokenized_datasets2"
      ],
      "metadata": {
        "id": "c2HlY1NGhlkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batching and dynamic padding\n",
        "from transformers import DataCollatorWithPadding\n",
        "collator = DataCollatorWithPadding(tokenizer = tokenizer)\n",
        "\n",
        "samples2 = tokenized_datasets2[\"train\"][:3]\n",
        "samples2 = {k: v for k, v in samples2.items() if k not in [\"idx\", \"sentence\"]}\n",
        "[len(x) for x in samples2[\"input_ids\"]]"
      ],
      "metadata": {
        "id": "O4anbWEVh3Ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = collator(samples2)\n",
        "{k: v.shape for k, v in batch.items()}"
      ],
      "metadata": {
        "id": "uhbZYvjqjGaK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}