{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umzVTSMgSLes"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "SlejsfqWXMXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PIPELINE is a function in transformers, that has a pre processor, model, and post processor\n"
      ],
      "metadata": {
        "id": "kE9tKoy8X5hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "-bYM33s0XOtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier([\"EU countries push for swift trade with trump\",\n",
        "            \"Gold prices are falling\"])"
      ],
      "metadata": {
        "id": "QyLW5MdyX4YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BEHIND THE PIPELINE**"
      ],
      "metadata": {
        "id": "9n890OCqGChf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   preprocessing needs to be done in exactly the same way as when the model was pretrained\n",
        "*   To do this, we use the AutoTokenizer class and its from_pretrained() method\n",
        "*   Using the checkpoint name of our model, it will automatically fetch the data associated with the model’s tokenizer and cache i\n",
        "\n"
      ],
      "metadata": {
        "id": "93nABe8qGxO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "sKEnTJZAbGMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_inputs = [\n",
        "    \"You are master of your soul \",\n",
        "    \"Captain of your life\"\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding = True, truncation = True, return_tensors = \"pt\")\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "F9WVceA4M6uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "to download a pretrained method Transformers provides an AutoModel class which also has a from_pretrained() method"
      ],
      "metadata": {
        "id": "I_qe8y9oOBUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "XL8CI8kHM6dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This architecture contains only the base Transformer module:\n",
        "--given some inputs, it outputs what we’ll call hidden states, also known as features. For each model input, we’ll retrieve a high-dimensional vector representing the contextual understanding of that input by the Transformer model.\n",
        "A high-dimensional vector?\n",
        "The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
        "\n",
        "Batch size: The number of sequences processed at a time (2 in our example).\n",
        "Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
        "Hidden size: The vector dimension of each model input.\n",
        "\n",
        "While these hidden states can be useful on their own, they’re usually inputs to another part of the model, known as the head. The different tasks could have been performed with the same architecture, but each of these tasks will have a different head associated with it."
      ],
      "metadata": {
        "id": "7QONShtbOpMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(**inputs)\n",
        "print(output.last_hidden_state.shape)"
      ],
      "metadata": {
        "id": "UKUJKtQ0M6SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the Transformer model is sent directly to the model head to be processed."
      ],
      "metadata": {
        "id": "jzK9Vxl8ahTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)"
      ],
      "metadata": {
        "id": "wt-RstrYM6DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.logits.shape)\n",
        "print(outputs.logits)"
      ],
      "metadata": {
        "id": "l0ZVitZcM51n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "6u3LCfZsbspR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "OUvYOs5efLrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.id2label"
      ],
      "metadata": {
        "id": "qlyLhJyufXk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can conclude that the model predicted the following:\n",
        "\n",
        "First sentence: NEGATIVE: 0.0402, POSITIVE: 0.9598\n",
        "Second sentence: NEGATIVE: 0.9995, POSITIVE: 0.0005"
      ],
      "metadata": {
        "id": "j1y5dy3EfnEH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1M809_c1fcc1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}